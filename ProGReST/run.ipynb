{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cu117.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_scatter-2.1.1%2Bpt20cu117-cp39-cp39-linux_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.1+pt20cu117\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu117.html\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_sparse-0.6.17%2Bpt20cu117-cp39-cp39-linux_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from torch-sparse)\n",
      "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.27.0,>=1.19.5 (from scipy->torch-sparse)\n",
      "  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, torch-sparse\n",
      "Successfully installed numpy-1.24.3 scipy-1.10.1 torch-sparse-0.6.17+pt20cu117\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu117.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_cluster-1.6.1%2Bpt20cu117-cp39-cp39-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/progress/lib/python3.9/site-packages (from torch-cluster) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/envs/progress/lib/python3.9/site-packages (from scipy->torch-cluster) (1.24.3)\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.1+pt20cu117\n",
      "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
      "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-bi097ur1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-bi097ur1\n",
      "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 5778c65686b0c6af40e745a3ec449e7b27628ead\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm (from torch_geometric==2.4.0)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/progress/lib/python3.9/site-packages (from torch_geometric==2.4.0) (1.24.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/progress/lib/python3.9/site-packages (from torch_geometric==2.4.0) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/progress/lib/python3.9/site-packages (from torch_geometric==2.4.0) (3.1.2)\n",
      "Collecting requests (from torch_geometric==2.4.0)\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m607.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing (from torch_geometric==2.4.0)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn (from torch_geometric==2.4.0)\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/progress/lib/python3.9/site-packages (from torch_geometric==2.4.0) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/progress/lib/python3.9/site-packages (from jinja2->torch_geometric==2.4.0) (2.1.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torch_geometric==2.4.0)\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests->torch_geometric==2.4.0)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->torch_geometric==2.4.0)\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->torch_geometric==2.4.0)\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn->torch_geometric==2.4.0)\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->torch_geometric==2.4.0)\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: torch_geometric\n",
      "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=921734 sha256=7f7fd0b7057791ef5b23b3e0ef2641974efcc14c5b79390bec2a94f30a458362\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-svlfrzup/wheels/e5/8a/bc/10228fa47bb01dd916740a9102c063c4e363e7dac6d55dba4a\n",
      "Successfully built torch_geometric\n",
      "Installing collected packages: urllib3, tqdm, threadpoolctl, pyparsing, joblib, idna, charset-normalizer, certifi, scikit-learn, requests, torch_geometric\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.1.0 idna-3.4 joblib-1.2.0 pyparsing-3.0.9 requests-2.28.2 scikit-learn-1.2.2 threadpoolctl-3.1.0 torch_geometric-2.4.0 tqdm-4.65.0 urllib3-1.26.15\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn_explain_mcts import explain_reg_mcts\n",
    "from model.prototree import GCNProtoSoftTree\n",
    "from utils.draw import draw_last_layer, draw_similarity\n",
    "from utils.featurizer import from_smiles\n",
    "from utils.train_test import test, train\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from args import get_args\n",
    "from features.GCN import GCN, GCNResidual\n",
    "\n",
    "from gcn_explain_mcts import explain_reg_mcts\n",
    "from model.prototree import GCNProtoSoftTree\n",
    "from utils.draw import draw_last_layer, draw_similarity\n",
    "from utils.featurizer import from_smiles\n",
    "from utils.train_test import test, train\n",
    "import torch.nn.functional as F\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "\n",
    "\n",
    "from Alchemy_dataset import TencentAlchemyDataset\n",
    "from torch_geometric.nn import GCNConv, Set2Set\n",
    "\n",
    "import pandas as pd\n",
    "from validate import get_results, get_valid_results, get_valid_targets, get_valid_dataset\n",
    "import sys\n",
    "# import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.16 ('progress')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n progress ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class x(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = x()\n",
    "args.features_size = 1024\n",
    "args.classes = 16\n",
    "args.depth = 5\n",
    "args.metric = 'mae'\n",
    "args.dataset_name = \"LD50\"\n",
    "\n",
    "args.lr = 0.005\n",
    "args.cuda = 0\n",
    "args.cl = 0.1\n",
    "args.ld = 0.01\n",
    "args.pt = 0.01\n",
    "\n",
    "args.epoch = 400\n",
    "args.batch_size = 64\n",
    "args.prototype_size = 128\n",
    "args.latent_distance = \"parents\"\n",
    "args.project_start = 250\n",
    "args.project_mod = 25\n",
    "args.warmup = 150\n",
    "args.join = 200\n",
    "args.split_seed = 1\n",
    "args.dataset_name = \"herg\"\n",
    "args.split_seed = 1\n",
    "args.split_method = 'scaffold'\n",
    "args.task_type = \"clst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/data-bin/processed/TencentAlchemy_dev.pt']\n",
      "['data/data-bin/processed/TencentAlchemy_valid.pt']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TencentAlchemyDataset(root='data/data-bin', mode='dev').shuffle()\n",
    "valid_dataset = TencentAlchemyDataset(root='data/data-bin', mode='valid')\n",
    "\n",
    "train_dataset.data.y = train_dataset.data.y[:,0].view(-1,1)\n",
    "\n",
    "valid_targets = get_valid_targets(root='data/data-bin/raw')\n",
    "valid_dataset = get_valid_dataset(valid_targets, valid_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "proj_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task_type = \"clst\"\n",
    "\n",
    "gnn_model = GCN(15, [256, 512, 512, 1024])\n",
    "\n",
    "features = gnn_model.features\n",
    "\n",
    "protoTree: GCNProtoSoftTree = GCNProtoSoftTree(features, args)\n",
    "protoTree.train()\n",
    "\n",
    "protoTree.join()\n",
    "lr = args.lr * 10\n",
    "lr_add, lr_features, lr_protos, lr_leaves = lr * 2 ** (-args.depth), lr * 2 ** (-args.depth), lr, lr\n",
    "\n",
    "parameters = [\n",
    "    {'params': protoTree.features.parameters(), 'lr': lr_features},\n",
    "    {'params': protoTree.add_on.parameters(), 'lr': lr_add},\n",
    "    #    {'params': protoTree.prototype_vectors, 'lr': lr_protos},\n",
    "    {'params': protoTree.leaves, 'lr': lr_leaves}\n",
    "]\n",
    "\n",
    "for i in range(protoTree.depth):\n",
    "    depth = protoTree.depth - i - 1\n",
    "    parameters.append({'params': protoTree.prototypes[i], 'lr': lr_protos * (2 ** (-depth))})\n",
    "\n",
    "optimizer = torch.optim.Adam(parameters)\n",
    "pt, cl,  ld = args.pt, args.cl, args.ld\n",
    "\n",
    "train_y = np.array([x.y.item() for x in train_loader.dataset])\n",
    "#  mean = train_y.mean()\n",
    "#  std = train_y.std()\n",
    "#  train_y = (train_y-mean)/std\n",
    "mean = 0\n",
    "std = 1\n",
    "kmeans = KMeans(n_clusters=args.classes).fit(train_y.reshape(-1, 1))\n",
    "labels = kmeans.predict(train_y.reshape(-1, 1))\n",
    "weights = []\n",
    "for i in range(args.classes):\n",
    "    weights.append(1/(len(labels[labels == i])/len(labels)))\n",
    "\n",
    "weights = torch.Tensor(weights)\n",
    "weights = weights/weights.max()\n",
    "#weights = weights/weights.norm()\n",
    "#criterion = nn.NLLLoss(weight=weights.to(device))\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s][W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "10it [00:26,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "total_parts_train, total_stats_train = train(protoTree, criterion, train_loader, optimizer, None,task_type=args.task_type, pt=pt, cl=cl, ld=ld, kmeans=kmeans, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "args.task_type = \"reg\"\n",
    "args.classes = 1\n",
    "mean = 0\n",
    "std = 1\n",
    "\n",
    "old_protoTree = protoTree\n",
    "leaves = F.softmax(old_protoTree.leaves, dim=1).cpu().detach().numpy()\n",
    "leaves = (leaves * kmeans.cluster_centers_.reshape(-1)).sum(-1).reshape(-1, 1)\n",
    "\n",
    "protoTree: GCNProtoSoftTree = GCNProtoSoftTree(features, args)\n",
    "for i in range(protoTree.depth):\n",
    "    protoTree.prototypes[i] = nn.Parameter(old_protoTree.prototypes[i].data, requires_grad=True)\n",
    "\n",
    "protoTree.leaves = nn.Parameter(torch.Tensor(leaves), requires_grad=True)\n",
    "# protoTree.to(device)\n",
    "\n",
    "lr = args.lr\n",
    "lr_add, lr_features, lr_protos, lr_leaves = lr * 2 ** (-args.depth), lr * 2 ** (-args.depth), lr, lr\n",
    "\n",
    "parameters = [\n",
    "    {'params': protoTree.features.parameters(), 'lr': lr_features},\n",
    "    {'params': protoTree.add_on.parameters(), 'lr': lr_add},\n",
    "    #    {'params': protoTree.prototype_vectors, 'lr': lr_protos},\n",
    "    {'params': protoTree.leaves, 'lr': lr_leaves}\n",
    "]\n",
    "\n",
    "for i in range(protoTree.depth):\n",
    "    depth = protoTree.depth - i - 1\n",
    "    parameters.append({'params': protoTree.prototypes[i], 'lr': lr_protos * (2 ** (-depth))})\n",
    "\n",
    "optimizer = torch.optim.Adam(parameters)\n",
    "protoTree.warmup()\n",
    "\n",
    "best = 999999\n",
    "best_proj = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch >= args.project_start and epoch % args.project_mod == 0:\n",
    "    protoTree.eval()\n",
    "    protoTree.project_fast_prototypes(proj_loader)\n",
    "    protoTree.train()\n",
    "    protoTree.last_only()\n",
    "\n",
    "    for proj_idx in range(15):\n",
    "        _, _ = train(protoTree, criterion, train_loader, optimizer, device, task_type=args.task_type, pt=pt, cl=cl, ld=ld,  mean=mean, std=std)\n",
    "\n",
    "    total_parts_eval, total_stats_eval = test(protoTree, criterion, valid_loader, optimizer, device, task_type=args.task_type, pt=pt, cl=cl, ld=ld, mean=mean, std=std)\n",
    "    total_parts_test, total_stats_test = test(protoTree, criterion, test_loader, optimizer, device, task_type=args.task_type, pt=pt, cl=cl, ld=ld, mean=mean, std=std)\n",
    "\n",
    "    if epoch >= args.project_start and total_stats_eval[args.metric] < best_proj:\n",
    "        best_proj = total_stats_eval[args.metric]\n",
    "        print(f\"BEST! {best_proj} - {total_stats_test[args.metric]}\")\n",
    "\n",
    "    torch.save(protoTree.state_dict(), f'./saves/{args.dataset_name}_{model_name}_proj.pt')\n",
    "\n",
    "\n",
    "    if epoch >= args.join:\n",
    "        protoTree.join()\n",
    "    else:\n",
    "        protoTree.warmup()\n",
    "\n",
    "\n",
    "    total_parts_train, total_stats_train = train(protoTree, criterion, train_loader, optimizer, device,  task_type=args.task_type, pt=pt, cl=cl,  ld=ld, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('progress')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3f33b0233e9019333b01be2feac53af536ed534a02af83b4c9a5d9a15c3a3d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

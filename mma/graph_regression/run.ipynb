{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/progress/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.nn.modules.module import T\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, global_add_pool\n",
    "from mma_conv import MMAConv\n",
    "import argparse\n",
    "import numpy as np\n",
    "from Alchemy_dataset import TencentAlchemyDataset\n",
    "from validate import get_valid_dataset, get_valid_targets\n",
    "import wandb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "args = x()\n",
    "\n",
    "args.seed=42\n",
    "args.epochs=200\n",
    "args.lr = 3e-4\n",
    "args.weight_decay = 5e-4\n",
    "args.hidden_dim=16\n",
    "args.out_dim=16\n",
    "args.edge_dim=16\n",
    "args.dropout = 0.5\n",
    "args.batch_size=16\n",
    "args.tower=1\n",
    "args.aggregators = \"mean,max,min\"\n",
    "args.scalers = \"identity,amplification,attenuation\"\n",
    "args.L=4\n",
    "args.mask = True\n",
    "args.property_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"run.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrandma\u001b[0m (\u001b[33mpgm_alchemy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/mma/graph_regression/wandb/run-20230428_054125-pao9ceuk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pgm_alchemy/alchemy/runs/pao9ceuk' target=\"_blank\">MMA_property0_epochs200</a></strong> to <a href='https://wandb.ai/pgm_alchemy/alchemy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pgm_alchemy/alchemy' target=\"_blank\">https://wandb.ai/pgm_alchemy/alchemy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pgm_alchemy/alchemy/runs/pao9ceuk' target=\"_blank\">https://wandb.ai/pgm_alchemy/alchemy/runs/pao9ceuk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pgm_alchemy/alchemy/runs/pao9ceuk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f01fc40b190>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "# Set the project where this run will be logged\n",
    "project=\"alchemy\", \n",
    "# We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "name=f'MMA_property{args.property_num}_epochs{args.epochs}', \n",
    "# Track hyperparameters and run metadata\n",
    "config={\n",
    "\"architecture\": \"MMA\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "['data-bin/processed/TencentAlchemy_dev.pt']\n",
      "['data-bin/processed/TencentAlchemy_valid.pt']\n",
      "Dataset loaded, you are sexy!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/progress/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#Load Alchemy Dataset\n",
    "\n",
    "print(\"Loading dataset\")\n",
    "train_dataset = TencentAlchemyDataset(root='data-bin', mode='dev').shuffle()\n",
    "valid_dataset = TencentAlchemyDataset(root='data-bin', mode='valid')\n",
    "\n",
    "# train_dataset.data.y = train_dataset.data.y[:,args.property_num]\n",
    "\n",
    "valid_targets =  get_valid_targets()\n",
    "valid_dataset = get_valid_dataset(valid_targets, valid_dataset)\n",
    "\n",
    "# valid_dataset.data.y = valid_dataset.data.y[:,args.property_num]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "print(\"Dataset loaded, you are sexy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = torch.zeros(5, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv = MMAConv(in_channels=75, out_channels=75,\n",
    "#                            aggregators=args.aggregators.split(\",\"), scalers=args.scalers.split(\",\"), deg=deg,\n",
    "#                            edge_dim=50, towers=5, pre_layers=1, post_layers=1,\n",
    "#                            mask = args.mask, divide_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load ZINC dataset\n",
    "# path = \"mma/data\"\n",
    "# train_dataset2 = ZINC(path, subset=True, split='train')\n",
    "# val_dataset2 = ZINC(path, subset=True, split='val')\n",
    "# test_dataset2 = ZINC(path, subset=True, split='test')\n",
    "\n",
    "\n",
    "\n",
    "# # # Create data loaders\n",
    "# train_loader2 = DataLoader(train_dataset2, batch_size=64, shuffle=True)\n",
    "# val_loader2 = DataLoader(val_dataset2, batch_size=64)\n",
    "# test_loader2 = DataLoader(test_dataset2, batch_size=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network class definition.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, aggregator_list, scaler_list):\n",
    "        \"\"\"\n",
    "        Initializes the neural network.\n",
    "\n",
    "        Args:\n",
    "        - args: Command line arguments.\n",
    "        - aggregator_list: List of aggregators.\n",
    "        - scaler_list: List of scalers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Define node and edge embeddings\n",
    "        # self.node_emb = Embedding(1000, 75)\n",
    "        # self.edge_emb = Embedding(100, 50)\n",
    "        self.node_layer = nn.Linear(15,75)\n",
    "        self.edge_layer = nn.Linear(4,50)\n",
    "\n",
    "        # Define aggregators and scalers\n",
    "        aggregators = aggregator_list\n",
    "        scalers = scaler_list\n",
    "\n",
    "        # self.layer1 = TransformerConv(in_channels=15,out_channels=1,edge_dim=4)\n",
    "        # GATConv(in_channels=15,out_channels=1)\n",
    "\n",
    "        # Define convolutional layers and batch normalization layers\n",
    "        self.convs = ModuleList()\n",
    "        self.batch_norms = ModuleList()\n",
    "        \n",
    "        # Add four convolutional layers with corresponding batch normalization layers\n",
    "        for _ in range(4): # 4 ------> number of layers\n",
    "            conv = MMAConv(in_channels=75, out_channels=75,\n",
    "                           aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                           edge_dim=50, towers=5, pre_layers=1, post_layers=1,\n",
    "                           mask = args.mask, divide_input=False)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(75))\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.mlp = Sequential(Linear(75, 50), ReLU(), Linear(50, 25), ReLU(),\n",
    "                              Linear(25, 12))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "        - x: Node features.\n",
    "        - edge_index: Edge indices.\n",
    "        - edge_attr: Edge attributes.\n",
    "        - batch: Batch indices.\n",
    "\n",
    "        Returns:\n",
    "        - The output of the neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # x = self.layer1(data.x,data.edge_index,data.edge_attr)\n",
    "\n",
    "        # x = self.node_emb(x.squeeze().long())\n",
    "        # edge_attr = self.edge_emb(edge_attr.long())\n",
    "\n",
    "        x = self.node_layer(x).unsqueeze(1)\n",
    "        edge_attr = self.edge_layer(edge_attr)\n",
    "\n",
    "        # print(x.shape,edge_attr.shape)\n",
    "        # Perform convolutional and batch normalization operations\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(batch_norm(conv(x, edge_index, edge_attr)))\n",
    "        \n",
    "        # Perform global pooling\n",
    "        x = global_add_pool(x, batch)\n",
    "        \n",
    "        # Perform fully connected layers\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize neural network, move to GPU if available\n",
    "model = Net(args, aggregator_list=args.aggregators.split(\",\"), scaler_list=args.scalers.split(\",\")).to(device)\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559/1559 [01:10<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 3.014693555425114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 18.34it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.004225089046877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.94it/s]\n",
      "100%|██████████| 1559/1559 [01:09<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.68925467081908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.72it/s]\n",
      "100%|██████████| 1559/1559 [01:09<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.4322726870668936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.90it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.2520698232479475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.95it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.1477757092467327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.85it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.0870392169227503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.92it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.0555695959434364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.72it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.0244351741287325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.90it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.0053779989041298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.89it/s]\n",
      "100%|██████████| 1559/1559 [01:08<00:00, 22.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.9913857810920901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:02<00:00, 20.87it/s]\n",
      " 32%|███▏      | 501/1559 [00:22<00:46, 22.65it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_stats = []\n",
    "valid_stats = []\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    total_train_mse = torch.zeros(12)\n",
    "    total_train_mae = torch.zeros(12)\n",
    "\n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        # loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss_matrix = ((out.squeeze() - data.y)**2)\n",
    "        train_mse = torch.mean(loss_matrix, axis=0).cpu()\n",
    "        loss = train_mse.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_mae = torch.mean(torch.abs(out.squeeze() - data.y), axis=0).cpu()\n",
    "\n",
    "        total_train_mse += train_mse\n",
    "        total_train_mae += train_mae\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_train_mse /= len(train_loader)\n",
    "    total_train_mae /= len(train_loader)\n",
    "    train_mse_stats = {f\"Train MSE property {i}\":train_mse[i].item() for i in range(train_mse.shape[0])}\n",
    "    train_mae_stats = {f\"Train MAE property {i}\":train_mae[i].item() for i in range(train_mae.shape[0])}\n",
    "    wandb.log(train_mse_stats)\n",
    "    wandb.log(train_mae_stats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"train loss\", total_loss/len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_val_mse = torch.zeros(12)\n",
    "    total_val_mae = torch.zeros(12)\n",
    "\n",
    "    for data in tqdm(val_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "    \n",
    "\n",
    "        loss_matrix = ((out.squeeze() - data.y)**2)\n",
    "        val_mse = torch.mean(loss_matrix, axis=0).cpu()\n",
    "        loss = val_mse.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        val_mae = torch.mean(torch.abs(out.squeeze() - data.y), axis=0).cpu()\n",
    "\n",
    "        total_val_mse += val_mse\n",
    "        total_val_mae += val_mae\n",
    "\n",
    "    total_val_mse /= len(val_loader)\n",
    "    total_val_mae /= len(val_loader)\n",
    "    val_mse_stats = {f\"Val MSE property {i}\":val_mse[i].item() for i in range(val_mse.shape[0])}\n",
    "    val_mae_stats = {f\"Val MAE property {i}\":val_mae[i].item() for i in range(val_mae.shape[0])}\n",
    "    wandb.log(val_mse_stats)\n",
    "    wandb.log(val_mae_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7392, 1.1109, 1.1208, 0.7673, 1.6818, 0.7486, 0.8198, 0.6573, 0.6853,\n",
       "         0.8563, 1.3191, 0.9090], device='cuda:0', grad_fn=<MeanBackward1>),\n",
       " tensor(11.4154, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mse, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559/1559 [01:14<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.11297978189815078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m ((out\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;241m-\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     49\u001b[0m valid_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     51\u001b[0m val_mse \u001b[38;5;241m=\u001b[39m mean_squared_error(out\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_stats = []\n",
    "valid_stats = []\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "\n",
    "    for data in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        # loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss = ((out.squeeze() - data.y)**2).mean()\n",
    "        loss.backward()\n",
    "        total_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse = mean_squared_error(out.squeeze().detach().cpu().numpy(),data.y.cpu().numpy())\n",
    "        train_mae = mean_absolute_error(out.squeeze().detach().cpu().numpy(),data.y.cpu().numpy())\n",
    "\n",
    "        train_stats['Train MSE'] += train_mse\n",
    "        train_stats['Train MAE'] += train_mae\n",
    "\n",
    "    train_stats['Train MSE'] /= len(train_loader)\n",
    "    train_stats['Train MAE'] /= len(train_loader)\n",
    "    wandb.log(train_stats)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"train loss\",sum(total_loss)/len(total_loss))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = []\n",
    "\n",
    "    for data in tqdm(val_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = ((out.squeeze() - data.y)**2).mean()\n",
    "        \n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "        val_mse = mean_squared_error(out.squeeze().detach().cpu().numpy(),data.y.cpu().numpy())\n",
    "        val_mae = mean_absolute_error(out.squeeze().detach().cpu().numpy(),data.y.cpu().numpy())\n",
    "\n",
    "        val_stats['Val MSE'] += val_mse\n",
    "        val_stats['Val MAE'] += val_mae\n",
    "    \n",
    "    val_stats['Val MSE'] /= len(val_loader)\n",
    "    val_stats['Val MAE'] /= len(val_loader)\n",
    "\n",
    "    wandb.log(val_stats)\n",
    "        \n",
    "\n",
    "    print(\"valid loss\",sum(valid_loss)/len(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('progress')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3f33b0233e9019333b01be2feac53af536ed534a02af83b4c9a5d9a15c3a3d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
